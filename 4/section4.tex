\chapter{Databases In Action}

In this section, we recreate dashboard example, which is used to explain limitations of ZODB, to interpolate complexity. Since we did a survey a great number of databases, we couldn't do it for each database. That's why strongest candidates are chosen from each category; namely, PostgreSQL, MongoDB, Redis and OrientDB.

\section{Relational Databases}

\subsection{PostgreSQL}

\begin{table}[H]
  \centering
  \caption{PostgreSQL users}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering}m{0.6in} | >{\centering}m{1.8in} | >{\centering\arraybackslash}m{2in} |}
	\hline
    \multicolumn{3}{|c|}{\textbf{users}}
    \\ \hline
    id & first\textunderscore name & last\textunderscore name
    \\ \hline
    1 & John & Doe
    \\ \hline
    2 & John & Smith
    \\ \hline
  \end{tabular}
  \label{postgresql-users}
\end{table}

\begin{table}[H]
  \centering
  \caption{PostgreSQL users and events association}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering}m{0.6in} | >{\centering}m{1.8in} | >{\centering\arraybackslash}m{2in} |}
	\hline
    \multicolumn{3}{|c|}{\textbf{users\textunderscore events}}
    \\ \hline
    user\textunderscore id & event\textunderscore id & role
    \\ \hline
    1 & John & Doe
    \\ \hline
    2 & John & Smith
    \\ \hline
  \end{tabular}
  \label{postgresql-events-users}
\end{table}

\begin{table}[H]
  \centering
  \caption{PostgreSQL events}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering}m{0.6in} | >{\centering}m{1.8in} | >{\centering\arraybackslash}m{2in} |}
	\hline
    \multicolumn{3}{|c|}{\textbf{events}}
    \\ \hline
    id & title & timestamp
    \\ \hline
    1 & CERN Open Days & 12345
    \\ \hline
    2 & CERN Higgs Event & 12365
    \\ \hline
    3 & TEDxCERN & 12377
    \\ \hline
  \end{tabular}
  \label{postgresql-events}
\end{table}

\subsubsection*{Conclusion}

PostgreSQL excels in this dashboard example while ZODB was failing miserably. Even if there is a need for strict schema, it's not a problem since this relationship is barely changing. The only concerning point is the necessity of a join when data from two tables is requested.

\section{Column Oriented-Databases}

\subsection{HBase}

\begin{table}[H]
  \centering
  \caption{HBase users}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering}m{0.6in} | >{\centering}m{0.8in} | >{\centering}m{1.8in} | >{\centering\arraybackslash}m{2in} |}
	\hline
    \multicolumn{4}{|c|}{\textbf{users}}
    \\ \hline
    row key & timestamp & \multicolumn{2}{|c|}{column families}
    \\ \hline
    user\textunderscore 1 & t1 & personal\textunderscore info:name="John Doe" & events:event\textunderscore 1="attendee", events:event\textunderscore 2="manager", events:event\textunderscore3="reviewer, attendee"
    \\ \hline
    user\textunderscore 2 & t2 & personal\textunderscore info:name="John Smith" & events:event\textunderscore 2="attendee"
    \\ \hline
  \end{tabular}
  \label{hbase-users}
\end{table}

\begin{table}[H]
  \centering
  \caption{HBase events}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering}m{0.6in} | >{\centering}m{0.8in} | >{\centering\arraybackslash}m{3.9in} |}
	\hline
    \multicolumn{3}{|c|}{\textbf{events}}
    \\ \hline
    row key & timestamp & column families
    \\ \hline
    event\textunderscore 1 & t5 & info:title="CERN Open Days", info:timestamp="12345"
    \\ \hline
    event\textunderscore 2 & t6 & info:title="CERN Higgs Event", info:timestamp="12365"
    \\ \hline
    event\textunderscore 3 & t7 & info:title="TEDxCERN", info:timestamp="12377"
    \\ \hline
  \end{tabular}
  \label{hbase-events}
\end{table}

We have tried to create the best possible schema as in tables \ref{hbase-users} and \ref{hbase-events}. There intrinsically is a relationship between users and events. Even if the data is small, it's a good example in order to show the difficulty of mapping relationships. Events can be de-normalized into users but Indico's schema contains chain of relationships with 4-5 levels where de-normalization doesn't scale.

\section{Document Oriented-Databases}

\subsection{MongoDB}

\begin{lstlisting}[language=HTML, caption=Example Dashboard in MongoDB]
  Collection 'users':
    [
      {
        "id": "user_1",
        "name": "John Doe",
        "events": [
          { "id": "event_1", "roles": ["attendee"] },
          { "id": "event_2", "roles": ["manager"] },
          { "id": "event_3", "roles": ["reviewer", "attendee"] }
        ]
      },
      {
        "id": "user_2",
        "name": "John Smith",
        "events": [
          { "id": "event_2", "roles": "attendee" }
        ]
      }
    ]

  Collection 'events'
    [
      {
        "id": "event_1",
        "title": "CERN Open Days",
        "timestamp": 12345
      },
      {
        "id": "event_2",
        "title": "CERN Higgs Event",
        "timestamp": 12365
      },
      {
        "id": "event_3",
        "title": "TEDxCERN",
        "timestamp": 12377
      }
    ]
\end{lstlisting}

\subsubsection*{Conclusion}

Event modification isn't uncommon task in Indico. If events are embedded into users collections, then in every change, users collection must be queried frequently, \textit{O(N)} at worst case. On the other hand, if events are put into a separate collection, then each modification can be handled in \textit{O(logN)}. That's why events should be in another collection while users are keeping references to events.

When data is divided into collections, writes involving both collections become problematic due to lack of ACID transactions. There are many relations that are more complex in Indico's schema compared to this simple example. Therefore, it seems not to be a good fit for write flow but with B-Tree indexes and ad-hoc query capability, pretty much every read query written in PostgreSQL is achievable. As a result, it makes sense to use it as a flexible cache when entities couldn't be embedded in another.

\section{Key-Value Store}

\subsection{Redis}

The \textit{Redis schema} (if fair to say) that was used for our implementation of the dashboard feature was the following:

\begin{lstlisting}[caption=Dashboard Schema in Redis]
  avatar_event_roles: (<AVATAR_ID>, <EVENT_ID>) -> SET(ROLE)
  event_avatars: <EVENT_ID> -> SET(AVATAR)
  avatar_events: <AVATAR_ID> -> SORTED_SET(EVENT_TS -> EVENT_ID)
\end{lstlisting}

The first namespace establishes a link between an avatar and an event, much like named edges of a graph. The second one (\texttt{event\_avatars}) maps a particular event to a set of avatars (so that we can have a quick list of all the people involved in a specific event), while \texttt{avatar\_events} does the opposite (maps an avatar to all the events it's connected to) with a little twist: events will be ordered by timestamp. This makes it easier to retrieve an ordered list of events relative to a particular user.

Here is the representation of the example Dashboard we've shown before using this schema:
\begin{lstlisting}[caption=Example Dashboard Data in Redis]
  avatar_event_roles: (user_1, event_1) -> attendee
                      (user_1, event_2) -> manager
                      (user_1, event_3) -> reviewer, attendee
                      (user_2, event_2) -> attendee

  event_avatars: event_1 -> user_1,
                 event_2 -> user_1, user_2
                 event_3 -> user_1

  avatar_events: user_1 -> 12345 -> event_1
                           12365 -> event_2
                           12377 -> event_3
                 user_2 -> 12345 -> event_2
\end{lstlisting}

There are a series of operations that need to be executed over this "schema":

\begin{itemize}
  \item \textbf{Adding a new role} (involves all the three namespaces)
  \item \textbf{Getting all the events in a specific time interval}, with the respective role information (involves \texttt{avatar\_events} and \texttt{avatar\_event\_roles})
  \item \textbf{Deleting an event} (involves all three namespaces)
  \item \textbf{Deleting an avatar} (involves all three namespaces)
  \item \textbf{Deleting a role link} (involves all three namespaces)
  \item \textbf{Updating event time} (involves \texttt{event\_avatars} and \texttt{avatar\_events})
  \item \textbf{Merging avatars} (involves all three namespaces)
\end{itemize}
In this particular implementation, most of the work is done client-side, using LUA scripts (similar to \textit{stored procedures}). This really speeds up query times, but has the inconvenience of locking the server, thus not allowing anything else to run in parallel. This can be problematic in the case of slow scripts.

The operations shown above have quite reasonable time complexity boundaries: Redis guarantees at least linear time in almost all operations, and most hash-related operations are logarithmic. \textit{Set} and \textit{get} of single values tend to be executed in constant time. A possible \textit{worst-case scenario} could be \textit{deleting an event} when all users are connected to all events. This would imply going over all users in \texttt{event\_avatars[deleted\_event]} and deleting them from \texttt{avatar\_events} (\textit{O(log(N)})) as well as \texttt{avatar\_event\_roles} (\textit{O(1)}). This results in \textit{O(Nlog(N))}.

\subsubsection{Conclusion}

Redis is a simple, fast and highly powerful key-value store. It is very versatile and can be used to solve a large set of problems. Its simplicity is, however, indicative of the role that it should occupy in an application stack. Its memory-oriented nature makes it ideal for caching and other use cases that require storing short-lived data (even though this data can be persisted) and the "key-value" philosophy that is behind it makes it hard to model complex relations using the simple data structures that are provided. There are sites successfully using Redis as their primary DB\footnote{\href{https://moot.it/blog/technology/redis-as-primary-datastore-wtf.html}{Redis as Primary Data Store}}, but the complexity of their DB schema cannot be compared to that of Indico's.

It is, then, clear that Redis would be a viable option in a "double database" scenario (primary DB using a different technology), in which it would act as a cache or possible primary storage for "simple" information that could be easily decoupled from the applications' business logic. Examples of that are:

\begin{itemize}
  \item Cached JSON objects (already used)
  \item Web Session information (already used, >=1.2)
  \item Cached templates
  \item Job scheduler tasks
  \item Plugin cache (plugins that are available, corresponding extension points...)
  \item Short URL registry
  \item Other possible cases could be:
  \begin{itemize}
    \item User account information
    \item Versioned minute storage
  \end{itemize}
\end{itemize}

\section{Graph Databases}

\subsection{OrientDB}

\begin{lstlisting}[language=SQL, caption=Dashboard Schema in OrientDB]
  (V, E) -> (Vertex, Edge)
  $ create class User extends V
  $ create class Event extends V
  $ create class Role extends E
  $ create vertex User set name = 'John Doe'
  $ create vertex User set name = 'John Smith'
  $ create vertex Event 
    set title = 'CERN Open Days', timestamp = '12345'
  $ create vertex Event
    set title = 'CERN Higgs Event', timestamp = '12365'
  $ create vertex Event
    set title = 'TEDxCERN', timestamp = '12377'
  $ create edge Role
    from
    (select from User where name = 'John Doe')
    to
    (select from Event where title = 'CERN Open Days')
    set roles = ['attendee']
\end{lstlisting}

\texttt{user} and \texttt{event} classes are vertexes and \texttt{role} class makes the edges of graph. \vspace{1cm}

\begin{lstlisting}[language=HTML, caption=Example Dashboard Data in OrientDB]
  Class 'User':
    [
      {
        "@rid": "#1:0",
        "name": "John Doe"
      },
      {
        "@rid": "#1:1",
        "name": "John Smith"
      }
    ]

  Class 'Event':
    [
      {
        "@rid": "#2:0",
        "title": "CERN Open Days",
        "timestamp": 12345
      },
      {
        "@rid": "#2:1",
        "title": "CERN Higgs Event",
        "timestamp": 12365
      },
      {
        "@rid": "#2:2",
        "title": "TEDxCERN",
        "timestamp": 12377
      }
    ]

  Class 'Role':
    [
      {
        "@rid": "#3:0",
        "@out": "#1:0",
        "@in": "#2:0",
        "roles": ["attendee"]
      },
      {
        "@rid": "#3:1",
        "@out": "#1:0",
        "@in": "#2:1",
        "roles": ["manager"]
      },
      {
        "@rid": "#3:2",
        "@out": "#1:0",
        "@in": "#2:2",
        "roles": ["reviewer", "attendee"]
      },
      {
        "@rid": "#3:3",
        "@out": "#1:1",
        "@in": "#2:1",
        "roles": ["attendee"]
      }
    ]

\end{lstlisting}

\subsubsection*{Conclusion}

OrientDB stores direct physical links to neighbours so when a record is retrieved, accessing its neighbours is \textit{O(1)} time. Therefore, all operations are guaranteed to complete in \textit{O(NlogN)} such that one vertex can be found in \textit{O(logN)} if proper index applied on the searched property and then, if this vertex has a relationship with every other vertex (such as a user has a role in every event), updating N vertexes will take \textit{O(N)}.

In small scale, as seen here, OrientDB performs well in any query but the number of vertexes and edges can easily go out of limits of one machine at the scale of Indico. When this project was started, OrientDB had even no replication feature, which is implemented (experimentally) in version 1.6 (December 2013). Therefore, until development of planned master-less replication and auto-sharding are complete (2.0), OrientDB is far from production scale.