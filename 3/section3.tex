\chapter{Survey of Candidates}

\par In this chapter, we try to look into as many databases as possible, in order to compare their strong and weak points and to find the best possible fit.

\section{Main Database Types}

Database systems specified in this document are mainly divided into 6 categories, which try to group current technologies according to the data structures and storage/querying strategies they employ:

\begin{enumerate}
  \item Object-Oriented
  \item Relational
  \item Column-Family
  \item Document-Oriented
  \item Key-Value
  \item Graph
\end{enumerate}

Each database type and possible candidates from each type are explained further.

\section{Object-Oriented Databases}

Object-Oriented databases bring the capabilities of object-oriented programming languages into the database world. Objects in programming languages can be directly stored, modified or replicated because these databases use the same object models as the corresponding programming languages. Unlike relational databases, there is no layer that converting objects back and forth. However, that requires database to be tightly integrated with programming language so that it provides a more transparent storage.

Object databases have been around since object-oriented programming was made popular by Smalltalk in 1970s. Although they have a long history, they become prominent with the Java "invasion" of software world. Even if the place of Java and object-oriented programming in software world are obvious, object-oriented databases have never been able to become mainstream like relational databases. As a result, there are not very successful products which leaves us with small number of choices to consider. ZODB and WakandaDB are the main viable systems. These are the ones that fit our context, not necessarily the most "viable". For instance, \textit{db4o}\footnote{\url{http://www.db4o.com/}} is a popular object database for Java and .NET but it's unusable in Python ecosystem.

\subsection{WakandaDB}

\vspace{-1.15cm} \hspace{4.1cm} \includegraphics[scale=0.8]{3/figures/wakandadb.png}

WakandaDB is very new - only half year passed since first public stable release at the time of writing. It pushes forward the motto \textit{JavaScript is everywhere}, which is a relatively recent phenomenon, pioneered by \textit{Node.js}.\footnote{\href{http://nodejs.org/}{A platform to build scalable network applications}, which is itself built on Chrome's runtime}

WakandaDB is an ambitious project because developers are trying hard to create a suite of tools that will work together seamlessly and will come handy in the life cycle of an application. Currently, WakandaDB comes with:
\begin{itemize}
  \item \textit{Wakanda Studio}: IDE\footnote{Integrated Development Environment} with full featured debugger
  \item \textit{Model Designer}: Easy creation of classes and schemas
  \item \textit{GUI Designer}: Makes it easier to create visual interfaces
  \item \textit{Web Application Framework}: Plug and play framework to use WakandaDB with a REST API
\end{itemize}

It is much more than a simple database product. This may be seen as an advantage since these tools save developers from tons of problems and details but it isn't actually perfect in terms of Indico. It forces its own custom stack but chosen database should coexist with other parts of Indico. Database change is already a big project in its own so tool chain and framework replacement aren't expected. Furthermore, its JavaScript push isn't nicely fits into Python stack of Indico but we should also note that JavaScript percentage in the code base of Indico has been increasing clearly with interface renovations.

If we were starting a web project now, WakandaDB seems a promising option with tooling and feature of totally embracing web. However, it is a bit dangerous even that situation since it's too early to play on WakandaDB as seen from lack of success stories and big deployments.

\begin{table}[H]
  \centering
  \caption{WakandaDB Object-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Dual license, AGPLv3 and commercial. \\ \hline

    Scalability \\ Replication &
    It's in infancy of development and stabilization.
    There aren't enough successful and big deployments.
    Since its target is specific, there are no benchmarks
    to compare its performance characteristics. \\ \hline
    
    Ease of Use Development &
    JavaScript is the only "first class" supported language.
    Server supports REST API so in theory every language
    can be leveraged via HTTP but Python isn't fully ready
    for the time being. \\ \hline

    Transactions Consistency &
    Transactions, even nested, are fully
    supported with the benefits of ACID guarantees. \\ \hline

    Community Momentum &
    New project, so it's normal that the community
    is just starting to get together but as
    seen from version control history, development
    isn't going on fast enough. Using only one language
    in every part of the stack is a huge time-saver and
    JavaScript is living its golden age so the choice of
    \textit{JavaScript Everywhere} makes sense, but
    results aren't obvious yet. \\ \hline

    Cost \\ Exit Strategy &
    Adapting the whole tool chain would be too much but
    if only the server component is utilized, cost can be reduced.
    Even using only the server part makes requires extensive JavaScript
    knowledge and leaving accumulated Python know-how.
    Exit would be easier compared to ZODB because there is an
    official MySQL connector which can export all data to be
    used in MySQL with little effort. \\ \hline

    Score &
    \rpt[3]{\FiveStar}\rpt[3]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{wakandadb}
\end{table}

\subsection{ZODB}

\vspace{-1cm} \hspace{2.9cm} \includegraphics[scale=0.6]{3/figures/zodb.png}

Firstly, \textsc{ZODB} is also considered as a candidate because if we couldn't find a better option that is worth of replacing, then we would continue with it and look for work around solutions to its problems.

The Zope Object Database is an object-oriented database that transparently persists Python objects.
In the beginning, it was just part of web application framework but later it is extracted to be used independently.

\textsc{ZODB} is a directed graph of Python objects where its starting vertex is a \textit{dictionary}\footnote{map, hash, etc.} called \textit{root}.
Objects must be attached directly or indirectly to the \textit{root} in order to be persisted.
Thus, persisted objects can be accessed by starting from \textit{root} and following links its children.

\textsc{ZODB} has powerful features in addition to being a transparent store:
\begin{enumerate}
  \item ACID transactions\footnote{A set properties: Atomicity, Consistency, Isolation and Durability}
  \item Version control for objects, which implies the need for periodic \textit{packing} operation. Every version of an object is stored, which dramatically increases database size. Packing operation includes taking back-up of database first so as to keep every version of the objects and then shrinking the database back to the bare minimum. Unless this operation is repeated regularly enough, disk space will easily be filled. Under heavy write traffic, this may be a serious problem and it does for Indico.  
  \item Pluggable storage - choice of different back-ends
  \begin{enumerate}
    \item File: Only one file in the file system and it's currently in use in most (if not all) Indico servers.
    \item Network aka. ZEO (Zope Enterprise Objects): It allows multiple client processes to access database concurrently which enables easier scaling and it also implements ACID but the ZEO server itself becomes a bottleneck and a single point of failure.
    \item RelStorage: Brings independence from one particular product at the expense of extra complexity and overhead. It allows replication and fault-tolerance which is good.
  \end{enumerate}
  \item Client caching: A partial solution for the lack of caching on the server side
  \item MVCC: Reads aren't blocked while only one write is allowed such that multiple writes are allowed as long as they don't cause version conflicts.
  \item Replication via \textit{ZRS} (Zope Replication Services): It was recently open sourced (May 2013). It was totally out-of-picture before due to its elevated price. ZRS recovers single point of failure by providing hot-backups for writes and load-balancer for reads.
\end{enumerate}

It has been around since late 90s, so it is reasonable to call \textsc{ZODB} as the most mature Python object database in the wild. It powers some successful products, Indico being one of them as well as Plone\footnote{\url{http://plone.org/}}.

One interesting drawback of \textsc{ZODB} being very tightly coupled with the code base is that each object is stored using its full class name on disk. That prevents moving classes around without a migration step. \textit{MaKaC}, Make a Conference, was the name that was chosen for Indico in its early development stages. As a result, source code still contains a lot of classes under \texttt{MaKaC} package or prefixed by \texttt{MaKaC}. They should be moved into \texttt{indico} package. The migration to a new back-end will hopefully help remove most (if not all) of these references.

\begin{table}[H]
  \centering
  \caption{ZODB Object-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Zope Public License \\ \hline
    Scalability &
    Although there are solutions like ZEO, RelStorage and ZRS (recently); lack of serve cache, ad-hoc query capability and indexing limits scalability. \\ \hline
    Ease of Use Development &
    ZODB is more like a library than a standalone program, which makes setup easy.
    It's tightly integrated with code, which is both an advantage and a disadvantage.
    While it makes things easier and most update functionality automatic, it can be at times tricky. A developer should know its inner workings in order to optimise performance. \\ \hline
    Transactions Consistency & ACID \\ \hline
    Community Momentum & Even if it's the most well-known and mature object database in the Python world, it's a niche project. The documentation and surrounding tool chain are not very complete despite its long history. \\ \hline
    Cost \\ Exit Strategy & Starting to use ZODB is easy at its simple setup and tight integration with Python. Getting rid of it can be very complicated. \\ \hline
    Score & \rpt[3]{\FiveStar}\rpt[3]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{zodb}
\end{table}

\section{Relational Databases} 

A relational database is a collection of tables of data items (\textit{rows}) that follow a relational model. Each table has a strict schema which specifies data parts, their types and their relationships to each others. Each row has a \textit{primary key}, composed of one or multiple columns, and which uniquely identifies itself. Relationships can be established within a table or between tables via a \textit{foreign key} which is a pointer to primary key of some table. Foreign keys provide various levels for normalization of tables. Normalization is a methodology that avoids data manipulation anomalies and loss of integrity by preventing redundancy and non-atomic values in table design. After the schema is normalized, which potentially makes it less intuitive compared to non-normalized data, arbitrary queries can be executed on the schema. This ad-hoc query capability is the most important feature of the relational world and nicely fits into evolving and agile software development. However, what is won by dividing data is lost when data spanning multiple tables is requested because data can only be recreated by joining tables, combining rows with referenced foreign keys, which are very costly in terms of performance. Even though normalization requires touching multiple tables at the same time to join data, relational database systems are fully ACID compliant with tunable guarantees at  e.g. row or column level. Furthermore, with the advent of big data requirements, replication is supported by default. Finally, schema design and normalization level are extremely important for sharding. Having both replication at default and sharding by default or application level, relational database can scale to relatively huge traffic.

Relational databases are some of the most battle-tested products of the last 20 years and have been well-studied for the last 40 years. As a consequence, there are multiple very successful deployments, and a huge active community with extensive know-how, and powerful tools.

\subsection{PostgreSQL}

\vspace{-1.15cm} \hspace{4.1cm} \includegraphics[scale=0.2]{3/figures/postgresql.png}

Some general properties are provided here but an explanation of in-depth features will be given later since PostgreSQL was the winner of our survey.

\begin{table}[H]
  \centering
  \caption{PostgreSQL Relational Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    PostgreSQL License (BSD or MIT-like) and clear roadmap and open development \\ \hline
    Scalability &
    Scalability was tested with successful deployments: Amazon, Dropbox, Heroku,
    Instagram, Reddit, Skype, NASA, Yahoo\\ \hline
    Ease of Use Development &
    Object-relational mapping (ORM) layer is needed but there is a well developed and supported Python library, SQLAlchemy\footnote{http://www.sqlalchemy.org/},
    which is in production at Dropbox, Yelp, Uber, OpenStack. \\ \hline
    Transactions Consistency & ACID \\ \hline
    Community Momentum &
    One of two large communities in the current database ecosystem, rivaling with MySQL. However, MySQL is losing blood since the acquisition of Sun Microsystems\footnote{\url{http://www.oracle.com/us/sun/index.html}} by Oracle\footnote{\url{http://www.oracle.com/index.html}} because main developers\footnote{Michael "Monty" Widenius} employed by Sun Microsystems left the company and forked the project such as MariaDB. Developers concern about strategy followed by Oracle and possible commercialization. Although many frameworks and PaaS providers support relational databases in general, they recommend PostgreSQL usage notably as a result of standards compliance. \\ \hline
    Cost \\ Exit Strategy & The integration of the ORM layer makes the code back-end agnostic unless back-end specific extensions are used. However, special features and types make the code more efficient, such that especially \textit{array} type enables ORM to load tree-like relations into Python collections. \\ \hline
    Score & \rpt[6]{\FiveStar} \\
    \hline
  \end{tabular}
  \label{postgresql}
\end{table}

\vspace{1.2cm}

\subsection{MySQL}

\vspace{-2.2cm} \hspace{3.2cm} \includegraphics[scale=0.5]{3/figures/mysql.jpg}

MySQL is the most popular relational database by far. MySQL owes its popularity to an investment in performance more than in features. PostgreSQL did it the other way around. This is a long story which has a dramatic end because PostgreSQL increased its performance in recent versions while MySQL has implemented most of the lacking features. Even if the gap is closing in simple queries, PostgreSQL performs better in complex queries involving sub-queries and/or multiple joins via its well-studied query optimizer.

There is a subtle detail in this comparison. PostgreSQL is an integrated database, a single block of code base but MySQL is composed of two layers, SQL (parser, optimizer, etc.) and storage layer (responsible for actual data storage, modification, etc.). Performance characteristics, features and transactional behaviour of storage layers are highly different. Thus, carefully establishing which storage is used is important for a fair comparison. In that respect, MySQL supports multiple storage engines; namely, InnoDB\footnote{\url{https://dev.mysql.com/doc/refman/5.7/en/innodb-storage-engine.html}}, BerkeleyDB\footnote{\url{http://www.aosabook.org/en/bdb.html}}, TokuDB\footnote{\url{http://www.tokutek.com/products/tokudb-for-mysql/}} and MyISAM\footnote{\url{http://dev.mysql.com/doc/refman/5.7/en/myisam-storage-engine.html}}, etc. but considering ACID requirements of Indico, development activity, and the need for a far comparison with PostgreSQL, InnoDB is our main concern.

\begin{table}[H]
  \centering
  \caption{MySQL with InnoDB vs PostgreSQL Comparison}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Winner} & \textbf{Features} \\
	\hline
    $=$ & MySQL (InnoDB) is very similar to PostgreSQL in terms of stored procedures, triggers, indexing and replication. \\ \hline
    PostgreSQL & Richer set of data types, better sub-query optimization, fully customizable default values and better constraints for foreign keys and cascades. Standard compliance. PostgreSQL has a clearer roadmap and a more permissive license compared to MySQL because, after acquisition by Oracle, MySQL has remained as open-source product but the community hasn't remained intact. As a solution, some concerned experienced MySQL developers forked the project, which resulted in multiple MySQL-like products in the market, such as MariaDB\footnote{\url{https://mariadb.org/}}, Drizzle\footnote{http://www.drizzle.org/}, etc. Moreover, with the help of MySQL's layered architecture, different companies are providing similar but diverse products which reuse the SQL layer and plug in a custom storage layer such as Percona Server, TokuDB, etc. As a result, many tailored products emerge but this divides community while PostgreSQL community as a whole has focused on making one code base perfect. \\ \hline
    MySQL & Advanced concurrency primitives like REPLACE - check and set atomically, and richer set options for horizontal partitioning. \\ \hline    
  \end{tabular}
  \label{mysql}
\end{table}

In short, MySQL is a very successful relational database that has a flexible design and is targeting performance at the expense of a full standard compliance and additional features. Its acquisition by Oracle in 2010 introduced, however, uncertainty regarding its long term future.

\begin{table}[H]
  \centering
  \caption{MySQL Relational Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Dual license (GPLv2 and commercial)\\ \hline
    Scalability &
    Scalability has been proven to be good, is used by companies such as Facebook, Google and Twitter, at a large scale.
    \\ \hline
    Ease of Use Development &
    An ORM layer is needed but code targeting for PostgreSQL in SQLAlchemy is generally usable by MySQL unless dialect-specific features of PostgreSQL are used extensively. \\ \hline
    Transactions Consistency &
    ACID in InnoDB \\ \hline
    Community Momentum &
	According to various rankings\footnote{\url{http://db-engines.com/en/ranking}}, MySQL is the most popular open source database with a still huge community. Its acquisition by Oracle has damaged its reputation for openness. Forks emerged and have benn following closely with upstream changes until now but after the stabilization of the long waited 5.6 version, first version to come out during the "NoSQL craze", forks started to diverge. MariaDB versions were one-to-one mappable with MySQL versions until 5.5 but since 5.6, they even chose to change their version scheme to start with 10.0.
	\\ \hline
    Cost \\ Exit Strategy &
    The ORM layer makes the replacement of a relational database easy, so what is said for PostgreSQL is also valid for MySQL. Their differences are getting subtle and unimportant. Moreover, MySQL can be easily replaced by any of its forks, at least while development is still synchronized with upstream.
    \\ \hline
    Score & \rpt[5]{\FiveStar}\rpt[1]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{mysql}
\end{table}

\subsection{SQLite}

\vspace{-1.15cm} \hspace{3cm} \includegraphics[scale=0.2]{3/figures/sqlite.jpg}

SQLite is an embedded relational database management system. In layman terms, it is a tiny C library that can be dynamically linked. Unlike MySQL or PostgreSQL, it's not a separate process that is waiting for requests. On the contrary, it is a part of the application and can be pictured as an independent module within the latter. and can be used through function calls.

All information related to the database is stored into a platform agnostic file. This is similar to the file storage of ZODB, and as by just copying that file, that same database can easily moved to wherever it is required.

Writes lock this file which means writes are executed sequentially, but reads don't require such a locking mechanism, which enables concurrent reads. If multiple writes happen concurrently, only one of them can gather the lock and others will fail with a specific error code. It's easy to see why it is much simpler than PostgreSQL and MySQL.

It's a self-contained, server-less\footnote{no daemon for waiting client connections}, zero-configuration relational database which makes it a universal solution for small needs. But it's not capable of serving high load in production. Since it's easy to use, it is used by many high caliber products such as Android and Firefox. In the context of Indico, it makes perfect sense for testing. However, many features provided by PostgreSQL and MySQL are missing in SQLite so, while keeping MySQL or PostgreSQL as main data back-end, using SQLite even for tests brings additional complexity in what contains back-end dependent operation such as date and time handling.

\begin{table}[H]
  \centering
  \caption{SQLite Relational Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Public Domain, Universal and even comes with Python. \\ \hline
    Scalability &
    A big problem because it is designed to be an embedded light database system. It may be thought as a testing-only solution since no configuration is required and the database can totally be kept in memory for better performance. The faster tests are runnable, the more frequently test are run.
    \\ \hline
    Ease of Use Development &
    An ORM layer is needed, since it's a relational database system and luckily, it is supported by SQLAlchemy. \\ \hline
    Transactions Consistency &
    ACID
    \\ \hline
    Community Momentum &
    Due to its popularity (Android, Chrome, Firefox, Opera, Skype, ThunderBird, etc.), the community is huge and it is well tested\footnote{\href{https://www.sqlite.org/testing.html}{more than 2 million tests are run for each release}} and documented.
    \\ \hline
    Cost \\ Exit Strategy &
    The ORM layer provides good abstraction. Although SQLite implements most of SQL standard, some important ones are omitted.\footnote{http://www.sqlite.org/omitted.html} such as triggers, full outer joins (left outer join is implemented and will be used in Indico), writable views and security (most vital lacking feature). We can somewhat live without these features but implementing Indico schema with exclusive database-level write lock doesn't scale. \\ \hline
    Score & \rpt[3]{\FiveStar}\rpt[3]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{sqlite}
\end{table}

\section{Column Family Databases}

Column-family databases are designed to handle large amount of data across many commodity servers at very large scale (e.g. Facebook). They provide auto-sharding and master-master replication so as to provide low latency and no single point of failure at the expense of dropping joins (favoring denormalization) and ACID guarantees. Column-family databases differentiate their records via a row id, as RDBMS does, timestamps for conflict resolution and/or versioning, column-family and column name. A column-family is a container for columns and columns can be added and deleted whenever needed without any down time. Moreover, every row generally has the same set of columns but it doesn't need to have same columns as RDBMS rows.

There are also column-oriented relational databases. They present data in terms of columns instead of rows because they physically store all data of one column together. Therefore, they are better in data-warehousing and customer relationship management (CRM). Except for this design difference, their features are more or less that of well-known row-oriented RDBMSs. A commercial product, Microsoft SQL Server, supports both storage designs, for instance. Vocabulary may be similar but column-oriented databases are very different designs than column-family databases.

\subsection{Cassandra}

\vspace{-1.15cm} \hspace{3.5cm} \includegraphics[scale=0.4]{3/figures/cassandra.jpg}

Apache Cassandra\cite{cassandracite} is an open source distributed (mainly) column oriented key-value store achieving high performance and availability. According to the CAP theorem\cite{cap}, a distributed system can't provide consistency, availability and partition tolerance at the same time. As a distributed database, Cassandra chooses to provide availability and partition tolerance (aka. distributed) with no single point of failure by the assigning same role to each node in the cluster. In addition, Cassandra supports master-less asynchronous inter-cluster replication for higher availability guarantees. At the same time, the third dimension, consistency, can also be adjusted according to the needs of particular applications. Since there is no difference between nodes, performance linearly scales as new nodes are added to the cluster, without interrupting existing nodes.

Due to its data model, Cassandra may actually be seen as a hybrid between key-value stores (inspired by Amazon Dynamo\cite{dynamo}) and column stores (inspired by Google BigTable\cite{bigtable}) because each row (on a record, a variable number of columns) is identified by a unique key which is distributed across the cluster using random partitioning or order preserving partitioning so that similar keys are closer to each other. Partitioning according to this key means assigning rows to physical nodes. Primary keys can be composed of multiple columns and after rows are partitioned by row id, they are clustered by the remaining columns.

Row partitioning requires a unique id to identify and to keep track of records. In the relational world:
\begin{itemize}
  \item Single server: a unique id is just an auto incrementing counter.
  \item Multiple servers: application logic, 3-rd party service or ticket servers. These options bring extra components and they may also be a single point of failure and write bottlenecks. It can also be seen as "escaping the problem" instead of solving it on the server side.
\end{itemize}

As seen, solving this problem in a distributed environment is hard. Relational options are used in master-slave fashion but Cassandra nodes are identical so this problem should be solved while keeping this architecture. Allowing each node to generate its own ids may result in inconsistencies and a lock is needed to prevent them. However, locking means waiting and performance degradation while the main objective of Cassandra is high throughput.

Cassandra tries to solve it not much differently than its relational counterparts:
\begin{itemize}
  \item It generates id from data
  \item It uses Universal Unique ID (UUID)\cite{uuid}, easily produced by the client
\end{itemize}

The structure of tables is pre-defined like relational databases but unlike RDBMS, they can be modified on the fly without any downtime (no blocking for queries). The number of columns can vary from 1 to 2 billion columns where each column has a name, value, timestamp and TTL (for expiration). A row doesn't need to contain exact set of columns.

In RDBMS, full normalization is recommended in order to prevent update anomalies. In Cassandra, there are higher level abstractions to store 1 to many relations in a column; namely, set, list and map. Cassandra doesn't support joins and sub-queries, except Hadoop tasks (Pig and Hive are also supported), so de-normalization is encouraged by using these higher level constructs.

\begin{table}[H]
  \centering
  \caption{Cassandra Column-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
	Apache Software License 2.0 and Cassandra is one of the most influential top level projects of Apache Software Foundation. \\ \hline
    Scalability &
    It's a clear winner in terms of throughput with increasing number of nodes by supporting clusters spanning multiple data centers with master-less replication. Especially, it's still perform well under heavy write load. Other databases requires locks for writes but Cassandra doesn't.
    \\ \hline
    Ease of Use Development &
    It's designed to handle very massive datasets and shines under a huge deployment with thousands nodes. Highly distributed nature requires some excellence to leverage it. However, its data model is highly similar to relational counterparts and supports collections, which are higher level primitives. For instance, one room can have multiple equipments. In relational, there is a need for a table that maps rooms and equipments since this is a many to many relationship. In Cassandra, this can easily be handled by \textit{set} collection. Moreover, de-normalization with collections is encouraged in Cassandra since there is no join. Therefore, converting deeply nested objects of Indico into Cassandra data model is easier than fully normalized relational tables.
    \\ \hline
    Transactions Consistency &
    There was no support for transactions but there are good efforts to bring and as a result, latest version supports lightweight transactions, which are far from ACID because it's only very specific case of generic transactions. Transactions enable to do multiple operations on multiple tables without any interference but lightweight transactions (very misleading naming) only support doing two things on the same row by a compare and swap operation.
    \\ \hline
    Community Momentum &
    Cassandra is the most popular column store.
    Companies that have high write loads like Facebook, Reddit and Twitter are users of Cassandra.
    \\ \hline
    Cost \\ Exit Strategy &
    Cassandra uses its own query language, \textit{CQL}, which isn't a standard SQL but pretty similar to SQL so making an intra-class change is much harder than other types of database systems. However, there are some projects recently to use Cassandra as a back-end such as MySQL/MariaDB and Titan, a graph database. Thus, transition to these products may require less effort.
    \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{cassandra}
\end{table}

\vspace{0.5cm}

\subsection{HBase}

\vspace{-1.8cm} \hspace{2.7cm} \includegraphics[scale=0.4]{3/figures/hbase.jpg} \vspace{-0.4cm}

Apache HBase is an open source clone of Google's BigTable\cite{bigtable}. It's a fault tolerant column oriented database with compression and version control for data. HBase is a CP system according to CAP Theorem.

Records are accessed, sorted and distributed by row key. A master node keeps tracks of assignments to slaves, \textit{region} servers, because records are sorted and an assignment is a continuous range so clients can easily learn where to look for data by knowing lower and upper bounds of regions.

It's easily thought that HBase isn't scalable due to the existence of a master node. On the contrary, the system is actually scalable because the master node isn't involved with data requests. On the other hand, reassignments of regions according to size (split or combine) and table operations require the master to be available. Thus, if master is down, data can be served by region servers because clients (can) cache the boundaries of region servers. However, a cold client can't learn assignments because assignments are kept in the meta table stored in the master which isn't up by then.

HBase runs on top of Hadoop Distributed File System (HDFS)\footnote{http://aosabook.org/en/hdfs.html} while BigTable runs of top of Google File System (GFS)\cite{gfs}. Adaptation phase of HBase is complex due to the HDFS setup and configuration but when it's there, replication of data comes for free with HDFS. HBase is designed to crunch very big data, peta-bytes of it. That's why HDFS requirement seems reasonable. But if we think about Indico, HBase is a much more complex solution than needed. It's basically "killing a mosquito with an atom bomb". It's in production in data-rich parts of high load systems such as Facebook, Twitter and Yahoo. 

Google has its own version, BigTable and it was very successful as a universal back-end at Google. Google is now working on the "next generation" of BigTable called \textit{Spanner}\cite{spanner} which is the storage engine used for Drive, GMail, Groups and Maps.

HBase doesn't support transactions but atomic operations are supported at the row level. ACID-like guarantees provided by relational databases are achievable via de-normalization. However, nesting data requires at least superficial knowledge of access patterns. Otherwise, ad-hoc queries would be a necessary but they aren't possible due to lack of transactions. The transaction problem is solved in the design phase with well-planned row key and nesting data into one row.

\begin{table}[H]
  \centering
  \caption{HBase Column-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    HBase may be the biggest kahuna of the systems that can handle massive data.
    HBase is also a top level project of Apache Software Foundation, licensed under Apache License 2.0 and it's a part of Hadoop ecosystem so runs on top of HDFS.
    \\ \hline
    Scalability &
    It follows the design of Google's BigTable and highly scalable.
    Like TaskTracker and JobTracker in Hadoop, or NameNode and DataNode in HDFS, HBase also has a master and slaves.
    Master handles administrative operations for auto-sharding and making assignments for slaves, region servers.
    This mapping is stored in META table at ZooKeeper node.
    Client need to know where each region is stored but they don't need to talk to master every time when they need data, instead clients can keep a cache and can directly connect to slaves.
    If there is a reassignment in regions, clients will get an exception to invalidate cache and will be required to relearn region assignment.
    Therefore, even if it seems it doesn't scale linearly due to a master-slave architecture, it is actually scales because master is only there to coordinate and master doesn't involve in data requests.
    However, there are some availability concerns related to master.
    If master is down, administrative operations can't be accomplished such as table creation/update but slaves can continue serving data operations even if master is down.
    \\ \hline
    Ease of Use Development &
    Since it's a part of Hadoop, it requires Hadoop machinery in place.
    Setup process of HBase is the most involving of all candidates.
    However, if the system is installed, it can easily handle peta-bytes of data and provides a giant table view of data to clients irrespective of where data is stored.
    \\ \hline
    Transactions Consistency &
    HBase isn't an ACID compliant database but it provides some of properties such as atomic mutation in the row level even if mutation includes multiple column families but that isn't enough in the context of Indico because Indico has a pretty complex schema and whole schema can't be nested so that one magic row key will enable atomic operations.
    \\ \hline
    Community Momentum &
    HBase is a successful product and has a lot of well-known users but migrations seem to be cumbersome.
    That's why users that don't really have (really) big data problems in the scale of HBase (tens of peta-bytes) are going away from HBase in the favor of simpler solutions.
    \\ \hline
    Cost \\ Exit Strategy &
    Entering and exiting from HBase are costly because it lives within Hadoop ecosystem and there is no direct replacement as powerful as HBase for very big data.
    \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{hbase}
\end{table}

\section{Document Oriented Databases}

Document-oriented databases are a middle point between key-value store and RDBMS.
Key-value stores identify records using a key and values aren't interpreted, only seen as a blob of bytes instead.
On the other hand, RDBMS requires a strict schema even for values ("columns" in relational terminology).
Document databases try to provide benefits of both world because they don't have a strict schema for values and can also give a meaning to the values so that they can index and query by values.

\subsection{MongoDB}

\vspace{-1.15cm} \hspace{3.6cm} \includegraphics[scale=0.2]{3/figures/mongodb.jpg}

MongoDB is the most popular document-oriented database.
It stores flexible JSON-style documents and provides a set of functionalities that is closer to that of RDBMS, namely a full-fledged query interface, full indexing support and in-place updates.
The schema of a MongoDB is basically non-existent - there is the concept of \textit{collections} that can be compared to relational tables, but objects can have whatever fields and values the programmer sees fit. An object is not constrained in any way just because it belongs to a particular collection.

\begin{table}[H]
  \centering
  \caption{MongoDB Document-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    AGPLv3\\ \hline
    Scalability &
    Replication and sharding comes at default which are cornerstones of a scalable system. MongoDB supports master-slave replication via replica sets to provide redundancy and higher availability.
    Auto-sharding is possible with configuration servers, sharded meta-data holders, and query routers, \textit{mongos}.
    Moreover, it is faster compared to competitors since it loosened some constraints. However, these weak guarantees can bite the application developer if application is very strict about data lost/consistency even under high load.
    MongoDB applies exclusive read-write lock so multiple reads can happen at the same time but a write requires a exclusive lock. There were problems related to the extent of write lock like being for whole \textit{mongod} instance but it is recently improved to be for a collection which should be improved for document level.
    \\ \hline
    Ease of Use Development &
    Python is one of the officially supported languages.
    MongoDB doesn't have a schema at all and very flexible. As a result, using MongoDB is a pleasure and development with MongoDB takes less time for the same feature in RDBMS.
    \\ \hline
    Transactions Consistency &
    MongoDB gives guarantee of BASE\cite{base}, eventual consistency\cite{eventually} and it doesn't support transactions which is a really bad disadvantage for Indico where there are multiple inter-relations between entities. Some parts of Indico nicely fit into a document but putting everything into a document is difficult in terms of design complexity and size constraint. Then, inter-relations discourage MongoDB as main storage.
    \\ \hline
    Community Momentum &
    Without any doubt, MongoDB is the most popular NoSQL database in the wild and it is very well documented.
    \\ \hline
    Cost \\ Exit Strategy &
    Replacing MongoDB with documented oriented databases wouldn't be much difficult because all of them; CouchDB, RethinkDB, RavenDB are similar at the core.
    Moreover, documents are close to objects as in ZODB. That's why coming from object-oriented world into document databases is a smoother path to follow.
    \\ \hline
    Score & \rpt[5]{\FiveStar}\rpt[1]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{mongodb}
\end{table}

\subsection{CouchDB}

\vspace{-1.15cm} \hspace{3.8cm} \includegraphics[scale=0.2]{3/figures/couchdb.jpg}

Apache CouchDB is a multi-master document-oriented database that is completely ready for the web.
From CAP Theorem, it's closer to AP system because it favors availability with its multi-master architecture while resolving conflicts are delegated to clients.
MongoDB vs CouchDB has caused many heated debates but this discussion actually can be reduced to whether consistency (CP) or availability (AP) is more important for the application.

Document model of CouchDB is very similar to the model of MongoDB. Documents, key-value pairs as in JSON, has a unique id to differentiate themselves.
Values can be primitive values but also more complex types like lists or associative arrays.

CouchDB doesn't support joins but can support ACID semantics in the document level via implementing MVCC not to block reads by writes.
Joins are possible via Map/Reduce tasks implemented as JavaScript functions.
This tasks are called views in CouchDB terminology and they can also be indexed and synchronized to updates to documents.

CouchDB provides eventual consistency in addition to availability and partition tolerance.
Each replica has its own copy of data, modifies it and sync bi-directional changes later.
This design brings offline support at default which may be very useful for smart phones since smart phones frequently go offline and come back a later time.

Other use case is running pre-define queries on top of occasionally changing data.
Compared to ad-hoc query capabilities of MongoDB, views of CouchDB require a bit more design beforehand.
However, if data structure is hardly changing, data is accumulating and versions of data are also important, then CouchDB is the perfect solution.

CouchDB is in production at Credit Suisse, dotCloud and Engine Yard but there are also some failure stories such as Ubuntu One.

\begin{table}[H]
  \centering
  \caption{CouchDB Document-Oriented Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    CouchDB, licensed under Apache License 2.0, is one of the high priority projects in Apache Foundation and more mature compared to other NoSQL prouducts because it has longer history and is developed by more inter-company developers.
    \\ \hline
    Scalability &
    CouchDB shines at scalability with incremental multi-master replication and also implements MVCC to prevent writes to block reads. Sharding may be done in application level but there are couple of products to extend CouchDB such as auto-sharding by CouchBase.
    \\ \hline
    Ease of Use Development &
    It is developed for web and provides HTTP API so can be easily used with any language.
    Data is stored in documents as MongoDB which is more natural and these documents are very flexible due to lack of schema.
    \\ \hline
    Transactions Consistency &
    ACID-like eventual consistency is guaranteed.
    This is far from the transactions of relational world but closer than MongoDB to what is needed by Indico.
    \\ \hline
    Community Momentum &
    Compared to MongoDB, CouchDB has a smaller community and it is preferred by companies in more niche categories such as PaaS providers while MongoDB is in production at eBay, Foursquare, New York Times and SourceForge.
    \\ \hline
    Cost \\ Exit Strategy &
    Coming into CouchDB from object-oriented world is easier than relational since documents are similar to objects.
    Consistency guarantees of CouchDB is more analogous to ZODB than MongoDB but this makes CouchDB replacement more difficult compared to MongoDB with other document databases.
    \\ \hline
    Score & \rpt[5]{\FiveStar}\rpt[1]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{couchdb}
\end{table}

\section{Key-Value Stores}

Key-value stores are distributed hash table implementation for larger data.
Each object has a unique key to be accessed with and values can be anything, seen as pure byte streams, so key-value stores don't enforce any schema.
Moreover, object databases can be as an example of key-value stores since value is just an object of a specific programming language.

Key-value stores don't try to interpret bytes of values.
As much as they give meaning to the value, they are closer to document-oriented databases.
Nature of design makes them highly scalable but only scalability isn't enough because ease-of-use, usage difficulty and quering capabilities are also important.
The more ad-hoc query capabilities key-value store has, the closer it is to document-oriented databases and the more usable it is in general so different products try to give a compromise in-between.

Due to simpler architecture, this is the category that has the most number of products.
Products are very different in terms of consistency characteristics, key structure, back-end and sorted-ness of keys. Main competitors are BerkeleyDB, Hazelcast, LevelDB, redis, Riak, Voldemort, memcached, Tarantool. Even document-oriented databases, MongoDB and CouchDB, can be seen as very capable key-value stores.

\subsection{Redis}

\vspace{-1.15cm} \hspace{3cm} \includegraphics[scale=0.2]{3/figures/redis.jpg}

Redis is a very simple key-value storage solution. It can be compared to memcached on steroids - a key-value store with extra [data structures] and additional features such as transactions and PubSub. Contrarily to e.g. memcached, it also allows developers to user server-side Lua scripts, akin to stored procedures. Redis will by default keep its data only in memory, but data can also be persisted on disk if desired. Redis Cluster is recently released to bring important features such as replication and strong consistency.

\begin{table}[H]
  \centering
  \caption{Redis Key-Value Store}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    According to rankings, Redis is the most popular key-value store in use and BSD license is suitable for Indico.
    However, Redis is still developed by only one developer, in the core.
    \\ \hline
    Scalability &
    Redis is developed as a single threaded in-memory database.
    Later, optional durability is added by flushing memory in intervals aka. snap-shotting or asynchronous append-only operation, \textit{journaling}.
    In December 2013, redis cluster, distributed version of Redis, is released but it is unstable and there is no success stories, at least for now.
    \\ \hline
    Ease of Use Development &
    Python is a well supported language.
    However, there is no rich data structure that can easily map objects of ZODB. Thus, converting complex hierarchy of objects to Redis primitives will be a pain and time taking.
    \\ \hline
    Transactions Consistency &
    Redis supports transactions but since it's single threaded, transactions are executed in a blocking fashion which deteriorates performance.
    It's also important to note that there is no roll-back for failed transactions.
    \\ \hline
    Community Momentum &
    Community size is medium since Redis can't be the only database system, it's more suited to work as a helper for another main storage engine but there are also a few successful only Redis deployments which have very very high write loads.
    Redis is in production at craiglist, flickr, stackoverflow.
    \\ \hline
    Cost \\ Exit Strategy &
    Using Redis is a bit difficult due to lack of complex structures but when time has come to exit, it is also difficult because Redis provides some powerful data structures compared to other key-value stores and if these structures are used, they may not be replaced easily.
    Moreover, main memory is the limiting factor for the size of the data because data managed by Redis can't be bigger than available memory.
    \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{redis}
\end{table}

\subsection{Riak}

\vspace{-1.15cm} \hspace{2.7cm} \includegraphics[scale=0.2]{3/figures/riak.jpg}

Riak is an open-source fault-tolerant key-value store inspired by Amazon Dynamo. It's very configurable in terms of trade-off proved by CAP Theorem.

Riak replicates data in master-less fashion into $n_val$ nodes which is \textit{three} at default. Where data will be written is achieved by consistent hashing. In case of node failure or network partition, keys can be written to neighbouring nodes (hinted hand-off) and when failing nodes are back, new nodes off-load their part and data, updated while they are unavailable, is rewritten to them.

Reads requires $R$ number of nodes to be read so cluster can tolerate $N - R$ node failures.

Consistent hashing enables distributing data evenly which makes query latency very predictable even in node failures. To make division more evenly, if there are low number of physical nodes, each Riak physical node creates \textit{vnodes} virtual nodes. Moreover, it also enables assigning different amount of data to nodes by changing \textit{vnodes} for the respective physical nodes if physical nodes have different specifications (more RAM, SSD, etc.). Riak is totally designed for distributed environment so generally, adding more nodes makes operations faster.

All nodes are equal and there is no master so any request can be served by any node. Consistency of data under concurrent writes is achieved by vector clocks.

Like CouchDB, Riak is written in Erlang and fully talks REST and supports MapReduce via JavaScript. In addition, Riak leverages Apache Solr to provide search capabilities and links to traverse objects via MapReduce to provide graph-like features. Moreover, even if Riak is a Key-value that requires access to objects via keys, it has more than that. Riak uses multiple backends; namely, memory, Bitcask, LevelDB and any combination of them together. If LevelDB or memory is in use, objects can queried by secondary indexes which are integer or string values to tag objects. Queries support exact match or range retrieval. Result can be paginated, streamed or even be given as input into MapReduce job.

Like HBase, Riak support inter-cluster replication but Riak has a master where HBase is master-less. Replication is done in two modes, real time and full-sync in 6 hours.

Riak is the choice of the majority of first 100 traffic websites such as GitHub (URL shortener and pages) and Google via acquisitions.

\begin{table}[H]
  \centering
  \caption{Riak Key-Value Store}
  \renewcommand{\arraystretch}{1.5}% Spread rows out...
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    It has Apache License which is suitable for Indico and it is one of most popular key-value stores with Redis.
    \\ \hline
    Scalability &
    Riak is inspired by Amazon Dynamo paper.
    It's developed for scalability and high availability so it is very easy to add/remove machines to cluster.
    Inter-cluster replication is even possible in master-slave fashion in two modes, asynchronous and synchronous.
    Intra-cluster, every piece of data is stored in multiple nodes and consistent hashing is used to divide the data between machines so Riak has very predictable latency.
    \\ \hline
    Ease of Use Development &
    Python is one of the officially supported languages but mapping complex object structures to very primitive data types is difficult.
    Moreover, lack of proper transactions like in relational databases puts burden on readers.
    \\ \hline
    Transactions Consistency &
    Riak implements vector clocks\cite{clocks} and reads use them to decide which write is the latest under concurrent updates so reads require extra work but writes always succeed.
    \\ \hline
    Community Momentum &
    Riak is in production at very popular websites due to its highly fault-tolerant nature which is vital for a business takes huge traffic, a result of being big. \\ \hline
    Cost \\ Exit Strategy & Entering into Riak as a main storage is difficult due to lack of complex data structures because complex structures of Indico should be converted into simple ones. There is no direct replacement of Riak in provided distributed fault tolerancy support so leaving Riak is also difficult. \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{riak}
\end{table}

\section{Graph Databases}

Graph Databases focus on graph structured datasets where adjacency of data is important and relations between entities make them closer to each other.
Graph databases try to optimize retrieval of these relations.

Graph databases are composed of two main parts; underlying storage and processing engine:

\begin{table}[H]
  \centering
  \caption{Architecture of Graph Databases}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{2in} | >{\centering}m{1in} | >{\centering\arraybackslash}m{1in} |}
	\hline
    \textbf{Graph DB} & \textbf{Storage} & \textbf{Engine} \\ \hline
	Twitter FlockDB & non-native & non-native \\ \hline
	neo4j & native & native \\ \hline
	OrientDB & native & native \\ \hline
	Titan & non-native & native \\ \hline
  \end{tabular}
  \label{orientdb}
\end{table}

Underlying storage may be a native graph storage which relations directly point to physical location of entities. This is very different than foreign keys in RDBMS because there is no need of computation to retrieve related entities since entity itself contains links to physical location of its neighbours which can be used in \textit{O(1)} time.

Computing engine implements standard graph algorithms on top of the storage engine. Performance and capabilities of this layer highly depends on storage. If storage is a native graph, graph can't be easily separated into parts which hoists scalability concerns. Even if database supports distributed architecture, whole data must be replicated to each node. However, having non-native storage enables processing giant graphs in the order of trillion edges via sharding capabilities of lower level storage, Cassandra or HBase, for instance. Unless graph partition quality is enough, performance characteristics can have noticeable differences since while native storage has whole data via \textit{O(1)} access, non-native storage may need to talk to many nodes over network.

Supporting a standard graph processing interface is important because it makes transition easier between databases according to changing needs of the domain. Therefore, even if there are graph databases with non-native graph processing in the market, they aren't listed (FlockDB is an exception due to being open source but it seems to be dead at the time of writing due to activity and Scala version, main development language) because they are commercial, old (first examples of graph databases) and have much smaller user community.

Graph databases seems to be de-facto go to database in the implementation of personal dashboard and recommendations, and also next generation of features of Indico such as complete socialization and gamification.

\subsection{OrientDB}

\vspace{-1.15cm} \hspace{3.5cm} \includegraphics[scale=0.2]{3/figures/orientdb.jpg} \vspace{0.3cm}

OrientDB is a hybrid between document-oriented and graph databases. By default, it works like a Document-oriented DB in which records are JSON-like documents. However, there is a \textit{graph} layer implemented on top of this that allows for elaborate relations to be established between documents. In this layer, both vertex and edges are documents that can be transparently manipulated.

There are 3 versions of OrientDB: 
\begin{enumerate}
  \item Standard (Document-oriented and Graph)
  \item Graph (TinkerPop Stack)
  \item Enterprise (coming with auto sharding/replication capabilities)
\end{enumerate}

Graph functionality is actually included in the standard edition - it's a common misunderstanding. The graph edition differs from the the standard one in the fact that it includes a TinkerPop stack. OrientDB is 100\% compliant with this graph processing stack. OrientDB can also be used as a key-value store, since document-orientation is a super-set of key-value stores. That is made possible by allowing records to be documents or flat strings. The data can be fully kept in memory if desired.

We have played a bit more with OrientDB compared to other databases since its features seemed to be a better fit for Indico.

\subsubsection*{Concepts}

There are some concepts that make OrientDB quite different from its relational companions:

\begin{itemize}
  \item \textbf{Record} - an instance of data as row in RDBMS or document in document-oriented databases.
  \item \textbf{Record ID} - auto-assigned unique number. It directly specifies the place of a record in disk, so it's not the equivalent to a logical ID (primary key) in RDBMS. That's why retrieving a record, when the ID is known, works in constant time, while it's O(log(n)) in RDBMS, using an index based on the id.
  \item \textbf{Cluster} - collection of links to records. A Record ID is composed of a Cluster ID and a sequence number within the cluster.
  \item \textbf{Class} - an abstraction of cluster used to group records. However, being a member of class doesn't put any constraints on records. Classes actually work in a similar way to their counterparts in Object-Oriented Programming and can bring new functionality via inheritance. By default, there is 1-to-1 mapping between classes and clusters and clusters can be seen as a table in an RDBMS that is used to group same type instances. However, in advanced usage, mapping may be n-to-m. Some examples:
  \begin{itemize}
	\item cluster \textit{person} to group all records from \textit{person} class (1-to-1)
	\item cluster \textit{cache} to group most accessed records (1-to-n)
	\item cluster \textit{car} to group all records belonging to the "car" class by type; suv, truck, etc. (n-to-1)
	\item cluster \textit{daily} to group all records by creation day (n-to-m)
  \end{itemize}
\end{itemize}

\subsubsection*{Security}

OrientDB has powerful security mechanism compared to other NoSQL stores; namely, rule (bitmask for CRUD), role (group of rules) and user (executor of rules). Rules can be defined server, database, cluster or record level. These features brings OrientDB closer to RDBMS, which are strict and powerful in  security.

\subsubsection*{Transactions}

Since we heavily rely on transactions, transaction support is important in a database. MongoDB allows atomic operations at the document level. However, we may need operations that span multiple documents if the data is to be split across collections. On the contrary OrientDB is fully ACID-compliant:
\begin{itemize}
  \item It allows multiple reads and writes on the same server
    \begin{itemize}
      \item client-scoped and no lock on the server
      \item implemented by a special property $@version$ tracked for each record. Mismatch means roll-back (MVCC).
    \end{itemize}
    \item Distributed transactions aren't supported. However, OrientDB got master-less replication via Hazelcast with latest release and as seen from code base, they have the base for distributed transactions. It's seen a must for horizontal scaling by the development team and planned for $2.0$ version. Thus, it's only a matter of time.
    \item Nested transactions aren't supported. It's highly probable that they will be implemented while distributed transactions.
\end{itemize}

\subsubsection*{Speed}

OrientDB seems to be really fast compared to other graph databases: 2 to 3 times faster than Neo4j in some benchmarks. In a fairly recent analysis (2012) by the Tokyo Institute of Technology and IBM, OrientDB was considered the fastest graph database from a group that included AllegroGraph, Fuseki and Neo4j (market leader).

Why is OriendDB fast?

\begin{itemize}
  \item Cache
  \begin{itemize}
    \item Level 1 - Thread Level (for each open connection) in client and Database Level in server
    \item Level 2 - JVM Level (shared between all connections) in client and Storage Level in server
    \item Advanced Storage cache depends on implementation
    \item OS cache for Memory Mapped files
  \end{itemize}
  \item Indexing
  \begin{itemize}
    \item Tree Index - MVRB-Tree (proprietary but open source algorithm that combines best parts of RB-Tree and B+ Tree). Tree Index is heavier and works in $O(logn)$ time but enables range queries.
    \item Hash Index - Lighter and works in \textit{constant time}.
    \item Composite Index (usable partially)
  \end{itemize}
  \item Hard Links
  \begin{itemize}
    \item Record ID is a pointer to a real physical place, not a logical concept so unlike RDBMS, it needs no calculation to access. As a result, Data loading is $O(1)$ time by record id.
  \end{itemize}
  \item Memory-mapped files
  \begin{itemize}
    \item Java New I/O (JSR 51 aka. NIO)
    \item No system calls, since there is no switch between kernel and user modes
	\item In-place update (\textit{seek} only required if a record is beyond the current page's boundaries)
	\item Lazy loading (efficient usage of memory)
	\item Adjusted page size via collected statistics about database
	\item The downside is that 32-bit, being only to address at most 4 Gb in their virtual address space, are limited in terms of file size.
  \end{itemize}
  \item Transactions
  \begin{itemize}
    \item No lock for reads (MVCC)
  \end{itemize}
  \item Tools to tweak
  \begin{itemize}
    \item explain (standard as PostgreSQL or MongoDB)
    \item profiler
  \end{itemize}
\end{itemize}

\subsubsection*{Scaling}

Scaling is done by multi-master scheme, each instance auto-magically synchronize each other. There are two modes:
\begin{itemize}
  \item synchronous: slower writes. The larger cluster size is, the slower writes because writes wait for each node to acknowledge success and quorum as in Cassandra or Riak is not configurable for now.
  \item asynchronous: faster but naturally weaker consistency
\end{itemize}

Each node has the whole database which may be a problem in some cases if database is so large (in a 64-bit system, it is very unlikely since virtual address space huge enough). Sharding is possible by grouping nodes and forming clusters according to shard keys but that possibly would bring some logic to client for whom to talk.

\subsubsection*{API}

Java and Scala are the main supported languages. However, Python is also fully supported by 3 drivers:

\begin{itemize}
  \item Native driver, a wrapper over C library (liborient)
  \item HTTP REST driver
  \item \textit{Rexster} driver (only if graph edition is in use)
\end{itemize}

There is a powerful console (native) with auto-complete that makes trying commands/concepts fairly easy. Moreover, there is another open-source project called OrientDB studio to accomplish administrative tasks easier like what \textit{PHPMyAdmin} does for MySQL/MariaDB.

% SHOULD KEEP?
The company that is supporting the development of database and providing database in the cloud as a service, is closed to focus more on the development side. Whether it is a good thing is a big mystery because it is more likely that they couldn't keep up with a sustainable business due to market share of OrientDB and very established competitors.

\subsubsection*{Extensibility}

There are two ways supported:
\begin{itemize}
  \item Server-side functions (different than triggers): totally generic functions that are also capable of executing sql queries on the database.
  \begin{itemize}
    \item Java
    \item JavaScript on Mozilla Rhino
  \end{itemize}
  \item Hooks (Java plug-ins to core)
\end{itemize}

\subsubsection*{Backup/Restore}

There is import/export mechanism to easily move the data/schema or upgrade. Automatic regular backup is native but it should be enabled.

There is also an emailing module which may be used for emergency, statistics, etc.

\subsubsection*{Competitors}

There are a lot of graph databases but there are a few in active development; namely, DEX, Neo4j and Titan. DEX is out of comparison since it is commercial. Neo4j is the leader. Titan is quite new but could catch up with others. Titan goes pretty good with other systems such as Cassandra, HBase, ElasticSearch, etc. which is very important in big data era, to be able to deploy a polyglot storage system. Moreover, it is getting popular faster than OrientDB by providing better scalability with underlying storage engine.

\begin{table}[H]
  \centering
  \caption{OrientDB Graph Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    OrientDB is licensed under Apache License 2.0.
    \\ \hline
    Scalability &
    With newest release in November 2013, OrientDB supports multi-master replication via Hazelcast but sharding is offloaded to application. Within graph databases with native graph storage, OrientDB seems to be the fastest but for a healthier evaluation, comparison with other databases from different disciplines is missing.
    \\ \hline
    Ease of Use Development &
    Python is supported by drivers provided from community (binary or HTTP). Being document and graph database enables flexible schema which is easier for refactoring and intuitiveness.
    \\ \hline
    Transactions Consistency &
    ACID with MVCC \\ \hline
    Community Momentum &
    OrientDB is getting popular fairly quickly because it isn't a native graph database.
    It's a hybrid of document and graph databases where each node or edge is also a document as in MongoDB or CouchDB.
    Thus, it is able to provide benefits of both worlds at the same time.
    However, there aren't enough successful big deployments which brings the question of if OrientDB is really production ready.
    \\ \hline
    Cost \\ Exit Strategy &
    If it is compared to Neo4j, picking up OrientDB is easier because parts that are problematic in Neo4j can be mapped via more intuitive document nodes and edges.
    \\ \hline
    Score & \rpt[5]{\FiveStar}\rpt[1]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{orientdb}
\end{table}

\subsection{Neo4j}

\vspace{-1.15cm} \hspace{3cm} \includegraphics[scale=0.2]{3/figures/neo4j.jpg}

Neo4j is the most popular and mature graph database which is deployed at Cisco, HP, Huawei, etc.
Neo4j adapts dual license, community and enterprise. There are huge differences between them in terms of scalability. Enterprise is fully ready for horizontal scalability with default cache, auto-sharding, master-slave replication where none of them available in community edition. Documentation and resources are plentiful compared other graph databases but lacking features between different versions risks accessibility.

Both versions support ACID transactions and leverage Lucene for fast searching. OrientDB supports documents everywhere so as to vertexes and edges can theoretically be annotated as it's requested. Neo4j delegates it with integrating Lucene. Since OrientDB does itself, it is faster but being an important Apache project, Lucene has much much faster development.

\begin{table}[H]
  \centering
  \caption{Neo4j Graph Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Neo4j is licensed under GPL and AGPL (for enterprise) and it is by far the most stable graph database in the market.
    \\ \hline
    Scalability &
    Scalability is lacking because horizontal scaling of a graph is really difficult due to relationships in a tightly connected graph. Neo4j Clustering technology provides horizontal scalability but only for enterprise version.
    \\ \hline
    Ease of Use Development &
    Some parts of Indico such as roles of users in events, links between conferences, sessions and contributions, etc. perfectly match links in a graph but not everything is mappable.
    \\ \hline
    Transactions Consistency &
    Neo4j has ACID transactions like relational databases. \\ \hline
    Community Momentum &
    Neo4j is the most popular graph product but in general, graph databases are niche products and mainly used by communication companies, where connections between entities are deadly important to business.
    \\ \hline
    Cost \\ Exit Strategy &
    Starting to use Neo4j isn't that difficult by close 1-to-1 mapping from ZODB objects to vertexes and edges of Neo4j.
    Graph databases support \textit{Gremlin}, a graph traversing and manipulation language like SQL in relational world.
    If Gremlin is used, Gremlin creates an abstraction layer on back-end storage like \textit{SQLAlchemy} does for relational databases so back-end can easily be changed with another Gremlin-compliant database such as OrientDB or Titan.
    \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{neo4j}
\end{table}

\subsection{Titan}

\vspace{-1.15cm} \hspace{3cm} \includegraphics[scale=0.2]{3/figures/titan.jpg}

Titan is a graph database that targets scalability with supporting scalable NoSQL databases. Graphs are connected so dividing graph into multiple physical nodes is extremely difficult without data-aware solutions. Thus, main scalability idea is to replicate the graph into multiple nodes via master-slave or multi-master scheme but this prevents scaling beyond capabilities of weakest node. While in terms of read query latency, this solution can provide reasonably good performance as Neo4j and OrientDB. While two unrelated points which are very far from each other are being updated, synchronization and locking are unnecessary. This is main problem Titan is trying to solve at the expense of losing ACID transactions. Moreover, scalability is more than throughput such that databases replicates whole graph in every node miserably fail scaling graphs in size. There are theoretical limits like 2 billion vertexes in Neo4j and OrientDB but these numbers can only be accessed in Titan practically.

In short, its architecture addresses an important problem which, in turn, draws attention of many users but it's still not widely used. Furthermore, the schema of Indico isn't directly mappable into a graph and while we are already trying to be away from column-family stores due to complexity, Titan even increases it by using them as pluggable storage.

\begin{table}[H]
  \centering
  \caption{Titan Graph Database}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{| >{\centering\bfseries}m{1in} | >{\centering\arraybackslash}m{4.5in} |}
	\hline
    \textbf{Criteria} & \textbf{Feature} \\
	\hline
    Availability &
    Apache License 2.0 \\ \hline
    Scalability &
    Titan has a notably different architecture compared to other graph databases via pluggable storage back-ends where scalable back-ends such as HBase or Cassandra brings their scalable nature into Titan.
    \\ \hline
    Ease of Use Development &
    Python is officially supported in binary protocol but while pluggable storage engine, which itself isn't easy to use, is bringing scalability, it also makes the system much more complex compared to OrientDB or Neo4j due to many moving parts.
    \\ \hline
    Transactions Consistency &
    ACID and eventual consistency are possible but it mainly depends on the back-end characteristics.
    \\ \hline
    Community Momentum &
    Graph databases are on the rise and data is getting bigger and bigger. Other graph databases can't divide their data but Titan can divide by following a different path. Thus, Titan can scale to the numbers OrientDB and Neo4j can't in terms of graph vertex and edge sizes. If GitHub popularity of graph databases is ranked (quite rational since all three are hosted on GitHub), then Titan seems to be the most popular one. However, this may be misleading because this popularity may be a result of popularity of pluggable storages.
    \\ \hline
    Cost \\ Exit Strategy &
	Start phase to use Titan may be probably difficult since required storage engine. Exit is comparably easier since Titan is also 100\% compliant with TinkerPop stack. If application is using TinkerPop stack, when graph database is changed, there is nothing to be changed from the application point of view.
    \\ \hline
    Score & \rpt[4]{\FiveStar}\rpt[2]{\FiveStarOpen} \\
    \hline
  \end{tabular}
  \label{titan}
\end{table}

\subsection{Broadness and Validity of Survey}

We try to give basic architecture details of each database but there are normally much more details to be considered. However, database ecosystem is quickly changing. We have seen multiple versions of same database even throughout this writing. That's why we tried to keep more tiny details online in a living document\footnote{\url{http://ferhatelmas.com/db-survey}}. This enables us to easily update data and since it gives comparison more exposure, small mistakes are found and fixed earlier.
